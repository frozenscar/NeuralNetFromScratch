{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8FLLr4fWc4htV7pkp+aua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frozenscar/NeuralNetFromScratch/blob/master/NeuralNetworkScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FPDFW1iISOND"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self,dims):\n",
        "      self.dims = dims\n",
        "      self.activations = []\n",
        "      self.init_weights()\n",
        "      self.dW = []\n",
        "\n",
        "    def init_weights(self):\n",
        "      self.W = []\n",
        "      for l in range(len(self.dims)-1):\n",
        "          self.W.append(torch.randn(self.dims[l]+1,self.dims[l+1]))\n",
        "\n",
        "    def printWeights(self):\n",
        "      print(\"Weights:\")\n",
        "      for i in range(len(self.W)):\n",
        "        print(self.W[i])\n",
        "\n",
        "    def printWeightsShapes(self):\n",
        "      print(\"Weights shapes:\")\n",
        "      for i in range(len(self.W)):\n",
        "        print(\"Weights \",i,self.W[i].shape)\n",
        "\n",
        "    def printActivations(self):\n",
        "      print(\"Activations:\")\n",
        "      for i in range(len(self.activations)):\n",
        "        print(\"Activation \",i,self.activations[i])\n",
        "\n",
        "\n",
        "    def printActivationsShapes(self):\n",
        "      print(\"Activations:\")\n",
        "      for i in range(len(self.activations)):\n",
        "        print(\"Activation \",i,self.activations[i].shape)\n",
        "\n",
        "\n",
        "    def printdWShapes(self):\n",
        "      print(\"dW:\")\n",
        "      for i in range(len(self.dW)):\n",
        "        print(\"dW \",i,self.dW[i].shape)\n",
        "\n",
        "    def add_bias(self,X):\n",
        "      return torch.cat((X,torch.ones(X.shape[0],1)),1)\n",
        "\n",
        "    def feedForward(self, X):\n",
        "        self.activations.clear()\n",
        "        for i in range(len(self.W)):\n",
        "            X = self.add_bias(X)\n",
        "            self.activations.append(X)\n",
        "\n",
        "            X = torch.mm( X, self.W[i])\n",
        "\n",
        "        return X\n",
        "\n",
        "    def lossFn(self,y,y_pred):\n",
        "      loss_gradient = 2*(y_pred-y)\n",
        "\n",
        "      return loss_gradient\n",
        "\n",
        "\n",
        "    def backProp(self,X,y):\n",
        "      self.dW.clear()\n",
        "      y_pred = self.feedForward(X)\n",
        "\n",
        "      dLdy = self.lossFn(y,y_pred)\n",
        "      dLdA = dLdy\n",
        "      for i in range(len(self.W)):\n",
        "        #IN the last layer We do not have the bias node\n",
        "        if i==0:\n",
        "          self.dW.insert(0,torch.mm(self.activations[-1-i].T,dLdA))\n",
        "\n",
        "        #We have bias nodes for all the remaining layers,\n",
        "        #Bias node is not associated with previous layers' weights\n",
        "        #Hence we remove the gradients associated with bias node.\n",
        "        else:\n",
        "          self.dW.insert(0,torch.mm(self.activations[-1-i].T,dLdA)[:,:-1])\n",
        "\n",
        "\n",
        "        if i==0:\n",
        "          dLdA = torch.mm(dLdA,self.W[len(self.W)-1-i].T)\n",
        "\n",
        "        #We will not use the error associated with bias node.\n",
        "        #because there are no other weights associated going backwards.\n",
        "        else:\n",
        "          dLdA = torch.mm(dLdA[:,:-1],self.W[len(self.W)-1-i].T)\n",
        "      return self.dW\n",
        "\n",
        "    def updateWeights(self,lr):\n",
        "      for i in range(len(self.W)):\n",
        "        self.W[i] = self.W[i] - lr*self.dW[i]\n",
        "\n",
        "    def train(self,X,y,lr,epochs):\n",
        "      for i in range(epochs):\n",
        "        self.backProp(X,y)\n",
        "        self.updateWeights(lr)\n",
        "    def predict(self,X):\n",
        "      return self.feedForward(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PNlNmqECSPKV"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork([2,5,1])\n",
        "#nn.printWeightsShapes()\n",
        "X=torch.tensor([[0,0],[0,1],[1,0],[1,1]])\n",
        "y=torch.tensor([[1],[0],[1],[0]])\n",
        "nn.feedForward(X)\n",
        "nn.printActivations()\n",
        "nn.printActivationsShapes()\n",
        "\n",
        "nn.backProp(X,y)\n",
        "nn.printWeightsShapes()\n",
        "nn.printdWShapes()\n",
        "nn.train(X,y,0.01,1000)\n",
        "y = nn.predict(X)\n",
        "\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Irt4AfboDZ",
        "outputId": "b29c65a2-ea19-440a-9ec4-e54018a2523d"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activations:\n",
            "Activation  0 tensor([[0., 0., 1.],\n",
            "        [0., 1., 1.],\n",
            "        [1., 0., 1.],\n",
            "        [1., 1., 1.]])\n",
            "Activation  1 tensor([[ 0.5327,  0.4884,  0.6641,  0.7048,  0.6526,  1.0000],\n",
            "        [-0.0748, -1.1302, -0.6394,  0.0470,  0.7450,  1.0000],\n",
            "        [ 1.9057,  0.1768,  0.6889,  0.4513,  0.7993,  1.0000],\n",
            "        [ 1.2982, -1.4417, -0.6145, -0.2066,  0.8916,  1.0000]])\n",
            "Activations:\n",
            "Activation  0 torch.Size([4, 3])\n",
            "Activation  1 torch.Size([4, 6])\n",
            "Weights shapes:\n",
            "Weights  0 torch.Size([3, 5])\n",
            "Weights  1 torch.Size([6, 1])\n",
            "dW:\n",
            "dW  0 torch.Size([3, 5])\n",
            "dW  1 torch.Size([6, 1])\n",
            "tensor([[ 1.0000e+00],\n",
            "        [-4.1723e-07],\n",
            "        [ 1.0000e+00],\n",
            "        [ 4.3213e-07]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUDHpZl3bwhb"
      },
      "execution_count": 163,
      "outputs": []
    }
  ]
}